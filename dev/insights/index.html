<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Insights · DeepViz.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">DeepViz.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">About</a></li><li><a class="tocitem" href="../gs/">Getting Started</a></li><li class="is-active"><a class="tocitem" href>Insights</a><ul class="internal"><li><a class="tocitem" href="#Primary-Attribution-1"><span>Primary Attribution</span></a></li><li><a class="tocitem" href="#Layer-Attribution-1"><span>Layer Attribution</span></a></li><li><a class="tocitem" href="#Others-1"><span>Others</span></a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li><li><a class="tocitem" href="../utility/">Utility Function</a></li><li><a class="tocitem" href="../usage/">Usage</a></li><li><a class="tocitem" href="../faq/">FAQs</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Insights</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Insights</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/AdarshKumar712/DeepViz.jl/blob/master/docs/src/insights.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Insights-1"><a class="docs-heading-anchor" href="#Insights-1">Insights</a><a class="docs-heading-anchor-permalink" href="#Insights-1" title="Permalink"></a></h1><p>How do neural networks manage to find patterns in data? How do they learn from existing data, and what do the inner layers of these artificial neural networks look like? All of this is studied under fields referred to as model interpretability and visualization, which are currently very active areas of research. This section provides an insight (or a detailed description) into various algorithms available with DeepViz to help understand the model better. These algorithms are further divided into subsections: <code>Primary Attribution</code>, <code>Layer Attribution</code> and <code>Others</code>.</p><h2 id="Primary-Attribution-1"><a class="docs-heading-anchor" href="#Primary-Attribution-1">Primary Attribution</a><a class="docs-heading-anchor-permalink" href="#Primary-Attribution-1" title="Permalink"></a></h2><p>These algorithms deal with the over model as a whole, thus attributing features of the overall model.</p><h4 id="Vanilla-Gradient-1"><a class="docs-heading-anchor" href="#Vanilla-Gradient-1">Vanilla Gradient</a><a class="docs-heading-anchor-permalink" href="#Vanilla-Gradient-1" title="Permalink"></a></h4><p>It’s the original saliency map algorithm for supervised deep learning from <a href="https://arxiv.org/abs/1312.6034">Simonyan et. al. (2013)</a>. It’s the simplest algorithm among gradient-based³ approaches and very fast to run, which makes it a great place to start to understand how saliency maps work.</p><p>The algorithms mainly consist of two steps:</p><ul><li>forward pass the image</li><li>backward pass to input layerto get the gradients for <code>top kth class</code> maximization.</li></ul><p>Once gradients are calculated for a particular class, these gradients can then be transformed into normalized heatmap, for better visualization</p><p>Useful Function(s): <a href="@ref"><code>viz_backprop</code></a>, <a href="@ref"><code>save_gradient_images</code></a>.</p><h4 id="Guided-Backpropagation-and-Deconvolution-1"><a class="docs-heading-anchor" href="#Guided-Backpropagation-and-Deconvolution-1">Guided Backpropagation and Deconvolution</a><a class="docs-heading-anchor-permalink" href="#Guided-Backpropagation-and-Deconvolution-1" title="Permalink"></a></h4><p>Guided backpropagation and deconvolution compute the gradient of the target output with respect to the input, but backpropagation of ReLU functions is overridden so that only non-negative gradients are backpropagated. In guided backpropagation, the ReLU function is applied to the input gradients, and in deconvolution, the ReLU function is applied to the output gradients and directly backpropagated.</p><p>For more details, check the original papers:</p><ul><li><a href="https://arxiv.org/abs/1412.6806"><code>Striving for Simplicity: The All Convolutional Net</code></a></li><li><a href="https://arxiv.org/abs/1810.03292"><code>Sanity Checks for Saliency Maps</code></a></li></ul><p>Useful Function(s): <a href="@ref"><code>viz_guidedbackprop</code></a>, <a href="@ref"><code>viz_deconvolution</code></a></p><h4 id="Integrated-Gradients-1"><a class="docs-heading-anchor" href="#Integrated-Gradients-1">Integrated Gradients</a><a class="docs-heading-anchor-permalink" href="#Integrated-Gradients-1" title="Permalink"></a></h4><p>Integrated gradients represents the integral of gradients with respect to inputs along the path from a given baseline to input. Here in this algorithm implementation, the path is chosen to be a straight line, and gradient is evaluated at equal intervals for the scaled images along the straight line. The cornerstones of this approach are two fundamental axioms, namely sensitivity and implementation invariance.</p><p>The algorithm consist of following steps:</p><ul><li>Scaling of Input Image with factor between 0.0 to 1.0 along the joing straight line at equal intervals </li><li>Calculating the gradient for each of the scaled image</li><li>Taking average of the gradients</li></ul><p>For more reference, check the original paper: <a href="https://arxiv.org/abs/1703.01365">Original Paper</a></p><p>Useful Function(s): <a href="@ref"><code>viz_integrated_gradients</code></a></p><h4 id="Gradient-x-Image-1"><a class="docs-heading-anchor" href="#Gradient-x-Image-1">Gradient x Image</a><a class="docs-heading-anchor-permalink" href="#Gradient-x-Image-1" title="Permalink"></a></h4><p>Gradient x Image is an extension of the saliency approach (vanilla gradient), taking the gradients of the output with respect to the input and multiplying by the input feature values. One intuition for this approach considers a linear model. the gradients are simply the coefficients of each input, and the product of the input with a coefficient corresponds to the total contribution of the feature to the linear model&#39;s output.</p><p>Useful Function(s): <a href="@ref"><code>grad_times_image</code></a></p><h2 id="Layer-Attribution-1"><a class="docs-heading-anchor" href="#Layer-Attribution-1">Layer Attribution</a><a class="docs-heading-anchor-permalink" href="#Layer-Attribution-1" title="Permalink"></a></h2><p>These algorithms deal with the specific layer of the model, thus attributing features of that specific layer of the model.</p><h4 id="GradCAM-1"><a class="docs-heading-anchor" href="#GradCAM-1">GradCAM</a><a class="docs-heading-anchor-permalink" href="#GradCAM-1" title="Permalink"></a></h4><p>GradCAM or Gradient weighted Class Activation Maximization uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. GradCAM is a layer attribution method designed for convolutional neural networks, and is usually applied to the last convolutional layer. GradCAM computes the gradients of the target output with respect to the given layer, averages for each output channel (dimension 2 of output), and multiplies the average gradient for each channel by the layer activations. The results are summed over all channels and a ReLU is applied to the output, returning only non-negative attributions.</p><p>For more details, refer to the paper: <a href="https://arxiv.org/abs/1610.02391"><code>Original Paper</code></a></p><p>Useful Function(s): <a href="@ref"><code>viz_gradcam</code></a>, <a href="@ref"><code>save_gradcam</code></a></p><h4 id="Guided-GradCAM-1"><a class="docs-heading-anchor" href="#Guided-GradCAM-1">Guided GradCAM</a><a class="docs-heading-anchor-permalink" href="#Guided-GradCAM-1" title="Permalink"></a></h4><p>Guided GradCAM computes the element-wise product of guided backpropagation attributions with upsampled (layer) GradCAM attributions. GradCAM attributions are computed with respect to a given layer, and attributions are upsampled to match the input size. This approach is designed for convolutional neural networks.</p><p>Guided GradCAM was proposed by the authors of GradCAM as a method to combine the high-resolution nature of Guided Backpropagation with the class-discriminative advantages of GradCAM, which has lower resolution due to upsampling from a convolutional layer.</p><p>For more details, refer to the paper: <a href="https://arxiv.org/abs/1610.02391"><code>Original Paper</code></a></p><p>Useful Function(s): <a href="@ref"><code>viz_guidedgradcam</code></a>, <a href="@ref"><code>save_gradcam</code></a></p><h2 id="Others-1"><a class="docs-heading-anchor" href="#Others-1">Others</a><a class="docs-heading-anchor-permalink" href="#Others-1" title="Permalink"></a></h2><p>These are algorithms which can not be properly assigned as primary attributions or layer attributions.</p><h4 id="SmoothGrad-1"><a class="docs-heading-anchor" href="#SmoothGrad-1">SmoothGrad</a><a class="docs-heading-anchor-permalink" href="#SmoothGrad-1" title="Permalink"></a></h4><p>SmoothGrad is a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. The core idea is to take an image of interest, sample similar images by adding Gaussian noise to the image (images in the neighborhood of image of interest), then take the average of the resulting sensitivity maps for each sampled image. The results suggest the estimated smoothed gradient, leads to visually more coherent sensitivity maps than the unsmoothed gradient, with the resulting visualization aligning better to the human eye with meaningful features.</p><p>For more details, refer to the original paper: <a href="https://arxiv.org/pdf/1706.03825.pdf"><code>Original Image</code></a></p><p>Useful Function(s) : <a href="@ref"><code>smooth_grad</code></a></p><h4 id="Image-Generator-1"><a class="docs-heading-anchor" href="#Image-Generator-1">Image Generator</a><a class="docs-heading-anchor-permalink" href="#Image-Generator-1" title="Permalink"></a></h4><p>This algorithms helps to visualise what type of input image maximizes the probability of <code>target class</code>. The idea behind Image Generator is simple in hindsight - Generate an input image that maximizes the model output activations for the <code>target class</code>.</p><p>Useful Function(s): <a href="@ref"><code>ImageGenerator</code></a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../gs/">« Getting Started</a><a class="docs-footer-nextpage" href="../api/">API Reference »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 5 June 2020 10:07">Friday 5 June 2020</span>. Using Julia version 1.0.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
